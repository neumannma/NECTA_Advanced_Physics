{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>README</u>\n",
    "\n",
    "\n",
    "## This file is designed to explain my project to transcribe and database Tanzania Advanced Physics NECTA questions.  Sections include:\n",
    "<ol> \n",
    "    <li> Background </li>\n",
    "    <li> Methodology </li>\n",
    "    <li> Python Files </li>\n",
    "</ol>\n",
    "\n",
    "##### * Note that this repository is incomplete and is continually being updated.  When finished, this message will be deleted *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.)  Background\n",
    "\n",
    "#### <b>Bio</b>\n",
    "My name is PJ Gibson and I was a Peace Corps Education Volunteer in Tanzania from July 2019 - March 2020 when all Peace Corps volunteers worldwide were evacuated because of the COVID-19 pandemic.  I taught Form 5 advanced level physics at Tosamaganga to over 200 bright Tanzanian boys aged anywhere between 18-23.  \n",
    "\n",
    "#### <b>Inspiration</b>\n",
    "As a teacher, I wished for a more accessible and organized online catalog of physics learning materials.  Specifically, I wanted an online database of previous Tanzanian NECTA examinations.  The NECTA is a national exam in Tanzania that has a large influence on your future life-path. Having the questions accessible online would allow teachers to easily share NECTA-level material with their students and help them adequately prepare for an objectively difficult final examination.  If organized by sub-topic, teachers could plan lessons around giving students the physics knowledge and english literacy skills to solve related NECTA-level questions, many of which appear more than once throughout the past few years.\n",
    "\n",
    "#### <b>Goal</b> \n",
    "I want to use the existing [Maktaba](https://maktaba.tetea.org/) resources to electronically transcribe advanced Physics NECTA examination questions.  After transcribing each question from all accessable years, I will tag each question with its respective: NECTA year, topic, sub-topic.  In cooperation with [Maktaba](https://maktaba.tetea.org/), I hope to make this sortable database publically accessable online.\n",
    "\n",
    "Potential additional tags include: respective syllabus objective, definition flag (0 or 1), marks.  These will be completed time-permitting.\n",
    "\n",
    "#### <b>Additional Notes</b> \n",
    "<ul>\n",
    "<li>\n",
    "    I would like to thank Peace Corps for the opportunity to allow me to volunteer teaching secondary education in Tanzania.  \n",
    "</li>\n",
    "<li>\n",
    "I would like to thank Maktaba by TATEA for the work they do to help provide accessable online study material for Tanzanian students.  \n",
    "</li>\n",
    "<li>\n",
    "No thanks to COVID-19 for causing us to evacuate.  That being said, stay smart and safe.\n",
    "</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.)  Methodology\n",
    "\n",
    "<ol>\n",
    "    <li>\n",
    "        Create the following folders:\n",
    "        <ul>\n",
    "            <li> csv_files </li>\n",
    "            <li> docx </li>\n",
    "            <li> finished_texts </li>\n",
    "            <li> OCR_outs </li>\n",
    "            <li> pdfs </li>\n",
    "            <li> python_files </li>\n",
    "        </ul>\n",
    "    <li> \n",
    "        Manually download all Advanced Physics NECTA questions from <a href=\"https://maktaba.tetea.org/past-exams/acsee/#physics\">Makataba</a>.  I put these into the <i>pdfs</i> folder.\n",
    "    </li>\n",
    "    <li> \n",
    "        Sift these question files through an online OCR tool geared to scrape text from picture files. I do this by running the python program 'PDF pics to text.ipynb'.  Let run until finished.  This will output text files to the <i>OCR_outs</i> folder.  \n",
    "    </li>\n",
    "    <li>\n",
    "        Manually edit the resulting text files by visually comparing them to the pictures.  This is tedious but faster than transcribing from scratch.  I edit by creating a .docx version of the text file simply because it is easier to read and it is always nice to have multiple copies of files in case of accidental slipups.\n",
    "    </li>\n",
    "    <li>\n",
    "        Save the finished, edited version to the folder called <i>finished_texts</i>.  Move the corresponding pdf to the <i>pdfs/finished</i> folder.  Move the corresponding OCR scraped text file to the <i>OCR_outs/finished</i>\n",
    "    </li>\n",
    "    <li>\n",
    "        After all tests are edited, use the python file called 'NECTA_onebyone.ipynb' to create a CSV that tags each question with its respective NECTA year, Topic, Subtopic.  Send this CSV file to the folder called <i>csv_files</i>.  \n",
    "    </li>\n",
    "</ol>\n",
    "\n",
    "<b> WHEN TRANSCRIBING QUESTIONS: </b>\n",
    "    <ul>\n",
    "        <li> I do not preserve numbering formatting, only raw questions. </li>\n",
    "        <li> In text files, different questions are seperated by 2 paragraphs.  Questions depending on one another or deemed to be extremely related are kept together as one multiple-part question. </li>\n",
    "        <li> I do not transcribe images/tables/diagrams.  Instead, each question depending on these will contain six hash marks - ######.  This unique tag will aid in cataloging them later, perhaps at the very end. </li>\n",
    "        <li> Each equation and variable within a problem is created using a consistant format that I may play with later on down the line to make look prettier </li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.)   Python Files\n",
    "\n",
    "Python is a computer programming language that is free and awesome.  You can use it to make your life easier in most imaginable situations.  For the sake of describing this project's python files, let's first start with two bits of relevant information.\n",
    "\n",
    "<b>OCR</b> - Web based Optical Character Recognition (OCR) tools are used to scrape text from images of papers.  Upon being fed an image, the tool scans for recognizable characters and attempts to recreate the image into copyable text. The one that I use is called [Free Online OCR](https://www.newocr.com/)\n",
    "\n",
    "<b>Selenium</b> - [Selenium](https://www.selenium.dev/) is a Python library that allows you to navigate websites.  As it says on the webpage, it automates browsers.  By recognizing patterns in the hidden HTML of a webpage, you can create a program using Selenium that automates an otherwise very tedious job.  Massive shouts out to Selenium, it's freaking awesome.\n",
    "\n",
    "\n",
    "#### <u>PDF pics to text.ipynb</u>\n",
    "\n",
    "<b>Input:</b>  Images of NECTA examinations.  These are stored in the <i>pdfs</i> folder and all have a file ending in .pdf even though many of them are simply images.\n",
    "\n",
    "<b>Process:</b>  Using Selenium, this program submits an examination file into an online OCR, waits for it to process, copies and saves the outputted scraped text, moves on to the next page of the examination until complete (can only run an OCR for one page).  After completing all pages, it moves onto the next examination until all examinations have been scraped.\n",
    "\n",
    "<b>Output:</b>  Text files with the same names as the examination original '.pdf' files, but now ending in '.txt' and saved to the <i>OCR_outs</i> folder.\n",
    "\n",
    "\n",
    "#### <u>NECTA_onebyone.ipynb</u>\n",
    "\n",
    "This file has not been practiced on much and will likely be edited further.\n",
    "\n",
    "<b>Input:</b>  An examination text file from the <i>finished_texts</i> folder.\n",
    "\n",
    "<b>Process:</b>  The program shows the user the text for a question.  The user must input the cooresponding sub-topic code. Sub-topic codes can be easily found using the 'subtopic_codes.odt' file within this same folder.  After submitting the sub-topic code, the program jumps to the next question until all are complete.  \n",
    "\n",
    "Human errors in sub-topic code labelling are troubleshooted at the end until all inputted codes match those in 'subtopics_and_codes.txt'.\n",
    "\n",
    "<b>Output:</b>  After going through and inputting each question's subtopic code, the program will output a CSV file to the <i>csv_files</i> directory.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
